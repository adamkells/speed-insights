# speed-insights

This repository hosts a tool that simplifies and accelerates the model comparison process. ðŸ”„

How It Works:

1. Provide Your Dataset: Input your dataset.
2. Bring Your Models: Include your pre-trained models.
3. Automated Insights: Easily generate metrics, visualizations, and insights to compare model performances.

This tool is designed to make model evaluation straightforward and efficient. ðŸ“ˆðŸ¤–

## Origin Story

The actual reason for building this repo was because I mostly work on regression tasks and constantly find myself frustrated with the kaggle-ization of work.
I find people often pick a metric like RMSE at random and then go all in on optimizing it. I like to view a range of metrics (rather than just one) and I often implement methods that examine the performance without aggregating. I was inspired particularly by Vincent Warmerdams [DoubtLab](https://github.com/koaning/doubtlab).

## Features

- Automated model comparison.
- Computes a variety of standard metrics.
- Easily extensible to include extra metrics.
- Returns data rows for deeper investigation.
- Automatically generates and saves wide range of useful visualisations.

## Table of Contents

- [Installation](#installation)
- [Usage](#usage)
- [Examples](#examples)
- [Contributing](#contributing)

## Installation

Provide instructions on how to install your package. Include any dependencies that users need to install and specify compatible Python versions.

```bash
pip install speed-insights
```

## Usage

```python
from yourpackage import ModelEvaluator

# Example usage
```

## Examples

```python
#examples
```

## Contributing

Contributions are more than welcome! Feel free to raise an issue if you have any suggestions for additional visualisations and metrics.




